##########################################################################
#######################      R          ##################################
##########################################################################
library(ChAMP)
library(ENmix)
library(preprocessCore)
library(tibble)
library(dplyr)

# Let's get started!
# Import the IDAT data files into ChAMP
myImport = champ.import(directory="path to folder containing IDAT files", arraytype="EPIC")

# filtering data from SNPs, Multi Hit probes, Sex chromosomes, etc
myLoad= champ.filter(arraytype="EPIC", ProbeCutoff=3, detP=0.01)

# data pre-processing using ENmix
beta = mpreprocess(myLoad$rgSet,nCores=6)

#   BMIQ normalisation
myNorm=champ.norm(beta=beta,rgSet=myLoad$rgSet,mset=myLoad$mset,resultsDir=".CHAMP_Normalization/",method="BMIQ",
	plotBMIQ=FALSE,arraytype="EPIC",cores=TRUE)

# Batch effect correction by ComBat
combatch= champ.runCombat(beta=myNorm, pd=myLoad$pd, variablename="Array", batchname=c("Batch"), logitTrans=TRUE)
myCombat = champ.runCombat(beta=myNorm,pd=myLoad$pd,batchname=c("Batch"))
champ.SVD(combatch,pd=myLoad$pd)

# Quantile_Normalisation
bqb = normalize.quantiles(combatch)
colname=colnames(combatch)
colnames(bqb)=colname
rowname=rownames(combatch)
rownames(bqb)=rowname

# Cell type Adjustment  
celltypenorm = champ.refbase(beta=bqb, arraytype="EPIC")
corrected_beta = celltypenorm$CorrectedBeta
cell_fraction = celltypenorm$CellFraction

# Our aim is to select CpG IDs residing on the TSS200, TSS1500, and 5'UTRs, which was obtained from hg19 annotation, You can use any relevant annotation for the selection of region of your choice 

# From that matrix, we want to filter out all probes which has beta value less than 0.2 at all timepoints
#Conversely, we aim to filter out the probes which has mean beta value greater 0.2 at atleast one time point
#This way we want to remove unmethylated probes
filt = your_matrix_with_selected_geneomic_region %>% filter_if(is.numeric, any_vars(.>0.2))

##########################################################################
#######################       Python          ############################
##########################################################################

import sys, csv, random, math, statistics
import numpy as np
infile = csv.reader(open(sys.argv[1],'r'), delimiter = ',')
outfile = csv.writer(open(sys.argv[2],'w'), delimiter = ',')


header = next(infile)
outheader = ['IlmnID','tb_varmean','ptb_varmean', 'stdev', 'stat','Q99','flag']
outfile.writerow(outheader)

for row in infile:
	cpg = row[0]
	beta = [float(x) for x in row[1:]]
	data = np.reshape(beta,(-1,4))
	var_all = np.var(data, axis=1)
	tb_varmean = np.mean(var_all[0::2])
	tbsd = statistics.stdev(var_all[0::2])
	ptb_varmean = np.mean(var_all[1::2])
	ptbsd = statistics.stdev(var_all[1::2])
	sdp = math.sqrt(((len(var_all[0::2])-1)*(tbsd**2)+(len(var_all[1::2])-1)*(ptbsd**2))/(len(var_all[0::2])-1+len(var_all[1::2])-1))
	stat = (ptb_varmean - tb_varmean)/sdp
	dist = []
	for i in range(0,1000):
		shuf_var = random.sample(list(var_all),len(var_all))
		stb_varmean = np.mean(shuf_var[0::2])
		sptb_varmean = np.mean(shuf_var[1::2])
		sdpn = math.sqrt(((len(shuf_var[0::2])-1)*(tbsd**2)+(len(shuf_var[1::2])-1)*(ptbsd**2))/(len(shuf_var[0::2])-1+len(shuf_var[1::2])-1))
		dist.append((sptb_varmean - stb_varmean)/sdpn)
	q99 = np.quantile(dist,0.99)
	flag = 'No'
	if stat > q99 : 
		flag = 'Yes'
	outfile.writerow([cpg,tb_varmean,ptb_varmean,sdp,stat,q99,flag])